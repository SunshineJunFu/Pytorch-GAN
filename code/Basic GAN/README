
Self-information:  a measure of information content

I(x) = - log(p(x)) 

p(x) = Pr(X=x) represents the probability that message x is choosen from all possible choices in message space X.

Entropy: a measure of the amount of uncertainty

H(X) = E[I(X)]

for discrete message space X:

H(X) = sum(-p(x)log[p(x)]) 

for continous message space X:

H(x) = integral(-p(x)log[p(x)]dx) 

Note: the bigger entropy means more uncertainty, more self-information. 

Kullback–Leibler divergence(information gain): measure the distance between two distributions, a "true" probability distribution p, and an arbitrary probability distribution q.

Dkl(p||q) =  Ep[log{p(x)/q(x)}]

Form for continous and discrete message space is the same as Entropy.

Note: the smaller KL means that the fake distribution is more closer to 'true' probability distribution.

Cross entropy:

CEH(p,q) = Ep[-logq] = H(p) + Dkl(p||q)

if distribution is regarded as "true" distribution, H(p) will be a constant. Hence, if we plan to minimize Cross entropy, it can be regarded as minimize KL divergence.

for binary discrete message space X:

CEH(p,q) = -p(x)log[q(x)] - [1-p(x)]log[1-q(x)]

PIPE Line

![](algorithm.GIF)

loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. In pseudo-code:

x.grad += dloss/dx
optimizer.step updates the value of x using the gradient x.grad. For example, the SGD optimizer performs:

x += -lr * x.grad
optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.

